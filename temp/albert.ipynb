{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch.nn as nn\n",
    "from transformers import AlbertModel, AlbertConfig, get_linear_schedule_with_warmup\n",
    "from transformers.modeling_bert import ACT2FN\n",
    "import torch\n",
    "from optimization import Lamb\n",
    "import argparse\n",
    "import os\n",
    "import easydict\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import pyxis.torch as pxt\n",
    "from torch.nn import CrossEntropyLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbertConsonantHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(config.embedding_size)\n",
    "        self.bias = nn.Parameter(torch.zeros(config.output_vocab_size))\n",
    "        self.dense = nn.Linear(config.hidden_size, config.embedding_size)\n",
    "        self.decoder = nn.Linear(config.embedding_size, config.output_vocab_size)\n",
    "        self.activation = ACT2FN[config.hidden_act]\n",
    "\n",
    "        self.decoder.bias = self.bias\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.activation(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states)\n",
    "\n",
    "        prediction_scores = hidden_states\n",
    "\n",
    "        return prediction_scores\n",
    "\n",
    "class Consonant(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Consonant, self).__init__()\n",
    "        self.config = config\n",
    "        self.albert = AlbertModel(config)\n",
    "        self.predictions = AlbertConsonantHead(config) \n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, answer_label=None):\n",
    "        outputs = self.albert(input_ids, attention_mask, token_type_ids)\n",
    "        sequence_output, pooled_output = outputs[:2]\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        \n",
    "        outputs = (prediction_scores, ) + outputs[2:]  \n",
    "        #print(prediction_scores.shape, answer_label.shape)\n",
    "        #print(prediction_scores.view(-1, self.config.output_vocab_size).shape, answer_label.view(-1).shape)\n",
    "\n",
    "        if answer_label is not None :\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            consonant_loss = loss_fct(prediction_scores.view(-1, self.config.output_vocab_size), answer_label.view(-1))\n",
    "            #consonant_loss = loss_fct(prediction_scores, answer_label)\n",
    "            #print(consonant_loss.shape, consonant_loss.mean())\n",
    "            total_loss = consonant_loss\n",
    "            outputs = (total_loss,) + outputs\n",
    "\n",
    "        return outputs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "albert_base_configuration = AlbertConfig(\n",
    "    hidden_size=256,\n",
    "    embedding_size=64,\n",
    "    num_attention_heads=4,\n",
    "    intermediate_size=1024,\n",
    "    vocab_size = 17579,\n",
    "    max_position_embeddings= 100,\n",
    "    output_vocab_size = 589,\n",
    "    type_vocab_size = 1,\n",
    ")\n",
    "\n",
    "model = Consonant(albert_base_configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_parser():\n",
    "    \n",
    "    parser = argparse.ArgumentParser(\"\")\n",
    "    #config setting\n",
    "    parser.add_argument('--hidden_size', default=256, type=int)\n",
    "    parser.add_argument('--embedding_size', default=64, type=int)\n",
    "    parser.add_argument('--num_attention_heads', default=4, type=int)\n",
    "    parser.add_argument('--intermediate_size', default=1024, type=int)\n",
    "    parser.add_argument('--vocab_size', default=17579, type=int)\n",
    "    parser.add_argument('--max_position_embeddings', default=100, type=int)\n",
    "    parser.add_argument('--output_vocab_size', default=589, type=int)\n",
    "    parser.add_argument('--type_vocab_size', default=1, type=int)\n",
    "    \n",
    "    #exp setting\n",
    "    parser.add_argument('--learning_rate', default=5e-4, type=float)\n",
    "    parser.add_argument('--weight_decay', default=0.01, type=float)\n",
    "    parser.add_argument('--adam_epsilon', default=1e-6, type=float)\n",
    "    parser.add_argument('--warmup_steps', default=10, type=int)\n",
    "    parser.add_argument('--train_batch_size', default=128, type=int)\n",
    "    parser.add_argument('--max_grad_norm', default=1.0, type=float)\n",
    "    parser.add_argument('--max_steps', default=200, type=int)\n",
    "    parser.add_argument('--save_checkpoint_steps', default=100, type=int)\n",
    "    parser.add_argument('--validation_step', default=50, type=int)\n",
    "    parser.add_argument('--save_log_steps', default=1, type=int)\n",
    "\n",
    "    parser.add_argument('--pretrain_dataset_dir', default='../dataset/processed/ratings_3_100', type=str)\n",
    "    parser.add_argument('--dataset_type', default='owt', type=str)\n",
    "    parser.add_argument('--exp_name', default='baseline', type=str)\n",
    "\n",
    "    parser.add_argument('--output_dir', default='output', type=str)\n",
    "    parser.add_argument('--gpus', default='0', type=str)\n",
    "    parser.add_argument('--n_gpu', default=1, type=int)\n",
    "    parser.add_argument(\n",
    "        '--gradient_accumulation_steps',\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help='Number of updates steps to accumulate before performing a backward/update pass.',\n",
    "    )\n",
    "    parser.add_argument('--seed', default=42, type=int, help='random seed for initialization')\n",
    "\n",
    "    parser.add_argument('--do_train', action='store_true')\n",
    "    parser.add_argument('--do_eval', action='store_true')\n",
    "\n",
    "    parser.add_argument('--num_workers', default=8, type=int)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dataloader(args):\n",
    "        \n",
    "    # We should filter out only directory name excluding all the *.tar.gz files\n",
    "    data_dir = os.path.join(args.pretrain_dataset_dir, 'train') \n",
    "    subset_list = [subset_dir for subset_dir in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, subset_dir))]\n",
    "    train_dataset = ConcatDataset([pxt.TorchDataset(os.path.join(data_dir, subset_dir)) for subset_dir in subset_list])\n",
    "\n",
    "    # Very small dataset for debugging\n",
    "    # toy_dataset = Subset(train_dataset, range(0, 100)) # -> If you want to make 100sample toy dataset. \n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.train_batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        args.opt, num_warmup_steps=args.warmup_steps, num_training_steps=args.max_steps\n",
    "    )\n",
    "    args.lr_scheduler = scheduler\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "    \"pretrain_dataset_dir\": '../dataset/processed/ratings_3_100',\n",
    "    \"train_batch_size\": 128,\n",
    "    \"num_workers\": 0,\n",
    "    \"warmup_steps\": 100,\n",
    "    \"max_steps\": 1000,\n",
    "    \"adam_epsilon\":1e-6,\n",
    "    \"weight_decay\":1e-8,\n",
    "    \"learning_rate\":1e-4\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "optimizer = Lamb(optimizer_grouped_parameters, lr=args.learning_rate, betas=(.9, .999), eps=args.adam_epsilon, adam=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['opt'] = optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = train_dataloader(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 100]) torch.Size([128, 100]) torch.Size([128, 100])\n"
     ]
    }
   ],
   "source": [
    "for batch in trainloader:\n",
    "    input_ids = batch['head_ids'].type(torch.LongTensor).cuda()\n",
    "    answer_label = batch['midtail_ids'].type(torch.LongTensor).cuda()  \n",
    "    attention_mask = batch['attention_masks'].type(torch.LongTensor).cuda()  \n",
    "    \n",
    "    print(input_ids.shape, attention_mask.shape,  answer_label.shape)\n",
    "    output = model(input_ids, attention_mask=attention_mask, token_type_ids=None, answer_label=answer_label)\n",
    "    output[0].backward()\n",
    "    optimizer\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 100])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1].argmax(dim=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005337603416066186"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.sum(answer_label==output[1].argmax(dim=2)).item() / torch.sum(answer_label!=-100).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2, device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(answer_label==output[1].argmax(dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3747, device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(answer_label!=-100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsonantAlbert(pl.LightningModule):\n",
    "    def __init__(self, hparams: argparse.Namespace, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hparams = hparams\n",
    "        self.config = config\n",
    "\n",
    "        #self.tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n",
    "        self.model = Consonant(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Log steps_per_sec every 100 steps\n",
    "        if self.global_step == 0:\n",
    "            self.last_time = time.time()\n",
    "\n",
    "        elif self.global_step % self.hparams.save_log_steps == 0:\n",
    "            steps_per_sec = self.hparams.save_log_steps / int(time.time() - self.last_time)\n",
    "            examples_per_sec = steps_per_sec * self.hparams.train_batch_size\n",
    "            self.logger.experiment.log_metric('steps_per_sec', self.global_step, steps_per_sec)\n",
    "            self.logger.experiment.log_metric('examples_per_sec', self.global_step, examples_per_sec)\n",
    "            self.logger.experiment.log_metric('learning_rate', self.global_step, self.lr_scheduler.get_last_lr()[-1])\n",
    "            self.last_time = time.time()\n",
    "\n",
    "        input_ids = batch['head_ids']\n",
    "        answer_label = batch['midtail_ids']\n",
    "        attention_mask = batch['attention_masks']\n",
    "\n",
    "        output = self.model(input_ids, attention_mask, answer_label)\n",
    "        # output = self.model(input_ids, attention_mask, token_type_ids, masked_lm_labels)\n",
    "\n",
    "        self.logger.experiment.log_metric('Total_loss', self.global_step, output[0].item())\n",
    "\n",
    "        # Save model and optimizer\n",
    "        if self.global_step % self.hparams.save_checkpoint_steps == 0 and self.global_step != 0:\n",
    "            \n",
    "            ckpt = f'ckpt-{self.global_step:07}.bin'\n",
    "            ckpt_dir = os.path.join(self.hparams.output_dir, ckpt)\n",
    "            \n",
    "            torch.save( {'model_state_dict': self.model.state_dict(), \n",
    "                         'optimizer_state_dict': self.opt.state_dict(),\n",
    "                         'scheduler_state_dict' : self.lr_scheduler.state_dict(),\n",
    "                         'loss': output[0].item()\n",
    "                        }, output_model_file)\n",
    "           \n",
    "            self.logger.log_artifact(ckpt_dir, ckpt_dir)\n",
    "        \n",
    "        return {'loss': output[0]}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch['head_ids']\n",
    "        answer_label = batch['midtail_ids']\n",
    "        attention_mask = batch['attention_masks']\n",
    "\n",
    "        output = self.model(input_ids, attention_mask, answer_label)\n",
    "        logits = output[1]\n",
    "        labels_hat = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        f1, exact = comput_score(answer_label, labels_hat)\n",
    "\n",
    "        output = OrderedDict({\n",
    "            \"val_loss\": output[0].item(),\n",
    "            \"f1\": f1,\n",
    "            \"exact\" : exact\n",
    "            \"batch_size\": len(answer_label)\n",
    "            })\n",
    "        return output\n",
    "\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        val_f1 = sum([out[\"f1\"] for out in outputs]).float() / sum(out[\"batch_size\"] for out in outputs)\n",
    "        val_exact = sum([out[\"f1\"] for out in outputs]).float() / sum(out[\"exact\"] for out in outputs)\n",
    "        val_loss = sum([out[\"val_loss\"] for out in outputs]) / len(outputs)\n",
    "        tqdm_dict = {\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_f1\": val_acc,\n",
    "                \"val_exact\": val_acc\n",
    "                }\n",
    "        result = {\"progress_bar\": tqdm_dict, \"log\": tqdm_dict, \"val_loss\": val_loss}\n",
    "        \n",
    "        self.logger.experiment.log_metric('val_loss', self.global_step, val_loss)\n",
    "        self.logger.experiment.log_metric('val_f1', self.global_step, val_f1)\n",
    "        self.logger.experiment.log_metric('val_exact', self.global_step, val_exact)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
    "\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = Lamb(optimizer_grouped_parameters, lr=args.hparams.learning_rate, betas=(.9, .999), eps=self.hparams.adam_epsilonm, adam=True)\n",
    "        #optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]\n",
    "\n",
    "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        self.lr_scheduler.step()\n",
    "\n",
    "    def get_tqdm_dict(self):\n",
    "        avg_loss = getattr(self.trainer, \"avg_loss\", 0.0)\n",
    "        tqdm_dict = {\"loss\": \"{:.3f}\".format(avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
    "        return tqdm_dict\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \n",
    "        # We should filter out only directory name excluding all the *.tar.gz files\n",
    "        data_dir = os.path.join(self.hparams.pretrain_dataset_dir, 'train') \n",
    "        subset_list = [subset_dir for subset_dir in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, subset_dir))]\n",
    "        train_dataset = ConcatDataset([pxt.TorchDataset(os.path.join(data_dir, subset_dir)) for subset_dir in subset_list])\n",
    "\n",
    "        # Very small dataset for debugging\n",
    "        # toy_dataset = Subset(train_dataset, range(0, 100)) # -> If you want to make 100sample toy dataset. \n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.hparams.train_batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=True,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=self.hparams.max_steps\n",
    "        )\n",
    "        self.lr_scheduler = scheduler\n",
    "        return data_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \n",
    "        # We should filter out only directory name excluding all the *.tar.gz files\n",
    "        data_dir = os.path.join(self.hparams.pretrain_dataset_dir, 'val') \n",
    "        subset_list = [subset_dir for subset_dir in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, subset_dir))]\n",
    "        train_dataset = ConcatDataset([pxt.TorchDataset(os.path.join(data_dir, subset_dir)) for subset_dir in subset_list])\n",
    "\n",
    "        # Very small dataset for debugging\n",
    "        # toy_dataset = Subset(train_dataset, range(0, 100)) # -> If you want to make 100sample toy dataset. \n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.hparams.train_batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=True,\n",
    "            shuffle=True\n",
    "        )\n",
    "        return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "\n",
    "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
    "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import argparse\n",
    "import shutil\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.logging.neptune import NeptuneLogger\n",
    "from pytorch_lightning.profiler import PassThroughProfiler, AdvancedProfiler\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    set_seed,\n",
    "    ElectraConfig,\n",
    ")\n",
    "\n",
    "\n",
    "from model import BaseElectra\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def revise_config(config: ElectraConfig, args: argparse.Namespace):\n",
    "    \"\"\"\n",
    "    Revise config as we want\n",
    "        1. Add multiplier between generator and discriminator\n",
    "        2. Degree of weight sharing\n",
    "            'no' : Share nothing\n",
    "            'embedding' : Share only embedding layer\n",
    "            'all' : Share all layers\n",
    "        3. Set configuration as electra-small\n",
    "    \"\"\"\n",
    "\n",
    "    config.multiplier_generator_and_discriminator = args.multiplier_generator_and_discriminator\n",
    "    config.weight_sharing_degree = args.weight_sharing_degree\n",
    "    config.rtd_loss_weight = args.rtd_loss_weight\n",
    "    config.generator_num_hidden_layers = args.generator_num_hidden_layers\n",
    "    config.save_log_steps = args.save_log_steps\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def make_parser():\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--mlm', default=True, action='store_true')\n",
    "    parser.add_argument('--mlm_probability', default=0.15, type=float)\n",
    "\n",
    "    parser.add_argument('--learning_rate', default=5e-4, type=float)\n",
    "    parser.add_argument('--weight_decay', default=0.01, type=float)\n",
    "    parser.add_argument('--adam_epsilon', default=1e-6, type=float)\n",
    "    parser.add_argument('--warmup_steps', default=10000, type=int)\n",
    "    parser.add_argument('--train_batch_size', default=128, type=int)\n",
    "    parser.add_argument('--max_grad_norm', default=1.0, type=float)\n",
    "    parser.add_argument('--max_steps', default=1000000, type=int)\n",
    "    parser.add_argument('--save_checkpoint_steps', default=25000, type=int)\n",
    "    parser.add_argument('--save_log_steps', default=100, type=int)\n",
    "\n",
    "    parser.add_argument('--pretrain_dataset_dir', default='../dataset/', type=str)\n",
    "    parser.add_argument('--dataset_type', default='owt', type=str)\n",
    "<<<<<<< HEAD\n",
    "    parser.add_argument(\"--model-name\", default='electra_small_owt',\n",
    "=======\n",
    "    parser.add_argument(\"--model-name\", default='electra_small_owt_reduce_log_metric',\n",
    ">>>>>>> 3ab8eedf19bbc47a4adc9d5fd69671ee451d0959\n",
    "                        help=\"The name of the model being fine-tuned.\")\n",
    "\n",
    "    parser.add_argument('--output_dir', default='output', type=str)\n",
    "    parser.add_argument('--n_gpu', default=1, type=int)\n",
    "    parser.add_argument(\n",
    "        '--gradient_accumulation_steps',\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help='Number of updates steps to accumulate before performing a backward/update pass.',\n",
    "    )\n",
    "    parser.add_argument('--seed', default=42, type=int, help='random seed for initialization')\n",
    "\n",
    "    parser.add_argument('--do_train', action='store_true')\n",
    "    parser.add_argument('--do_eval', action='store_true')\n",
    "\n",
    "    parser.add_argument('--num_workers', default=8, type=int)\n",
    "    parser.add_argument('--generator_num_hidden_layers', default=12, type=int)\n",
    "    parser.add_argument('--discriminator_num_hidden_layers', default=12, type=int)\n",
    "\n",
    "    parser.add_argument('--multiplier_generator_and_discriminator', default=4, type=int)\n",
    "    parser.add_argument('--weight_sharing_degree', default='embedding', type=str)\n",
    "    parser.add_argument('--rtd_loss_weight', default=50.0, type=float)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "def main():\n",
    "\n",
    "    args = make_parser()\n",
    "\n",
    "    config = CONFIG_MAPPING['electra']()\n",
    "    config = revise_config(config, args)\n",
    "\n",
    "    logger.info('Electra config %s', config)\n",
    "    logger.info('Training args %s', args)\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(args.seed)\n",
    "    if not os.path.exists(os.path.join('../', args.output_dir)):\n",
    "        os.mkdir(os.path.join('../', args.output_dir))\n",
    "\n",
    "    # Initialize model directory\n",
    "    # model_dir = \"{}_{}\".format(args.model_name, datetime.datetime.now().strftime(\"%y-%m-%d_%H:%M:%S\"))    \n",
    "    args.output_dir = os.path.join('../', args.output_dir, args.model_name)\n",
    "    if os.path.exists(args.output_dir):\n",
    "        flag_continue = input(f\"Model name [{args.model_name}] already exists. Do you want to overwrite? (y/n): \")\n",
    "        if flag_continue.lower() == 'y' or flag_continue.lower() == 'yes':\n",
    "            shutil.rmtree(args.output_dir)\n",
    "            os.mkdir(args.output_dir)\n",
    "        else:\n",
    "            print(\"Exit pre-training\")\n",
    "            exit()\n",
    "    else:\n",
    "        os.mkdir(args.output_dir)\n",
    "\n",
    "    model = BaseElectra(args, config)\n",
    "    \n",
    "    neptune_api_key = os.environ['NEPTUNE_API_TOKEN']\n",
    "    neptune_project_name = 'IRNLP/electra'\n",
    "    neptune_experiment_name = 'electra_pytorch'\n",
    "\n",
    "    neptune_logger = NeptuneLogger(\n",
    "        api_key=neptune_api_key,\n",
    "        project_name=neptune_project_name,\n",
    "        experiment_name=neptune_experiment_name,\n",
    "        tags=[\"torch\", \"pretrain\"],\n",
    "    )\n",
    "\n",
    "    train_params = dict(\n",
    "        gpus=args.n_gpu,\n",
    "        gradient_clip_val=args.max_grad_norm,\n",
    "        logger=neptune_logger,\n",
    "        early_stop_callback=None,\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(profiler=False, **train_params)\n",
    "    if args.do_train:\n",
    "        trainer.fit(model)\n",
    "\n",
    "    return\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.venv/torch1.4.0-py3.6-cuda10.1/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import argparse\n",
    "\n",
    "import pyxis.torch as pxt\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    ElectraPreTrainedModel,\n",
    "    ElectraForMaskedLM,\n",
    "    ElectraForPreTraining,\n",
    "    ElectraTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from data_collator import DataCollatorForLanguageModeling\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import ConcatDataset, Subset\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "\n",
    "BertLayerNorm = torch.nn.LayerNorm\n",
    "\n",
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None):\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        else:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand(input_shape)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = inputs_embeds + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "class ElectraEmbeddings(BertEmbeddings):\n",
    "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.embedding_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.embedding_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.embedding_size)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.LayerNorm = BertLayerNorm(config.embedding_size, eps=config.layer_norm_eps)\n",
    "\n",
    "\n",
    "class Electra(ElectraPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.discriminator_config = copy.deepcopy(config)\n",
    "        self.generator_config = copy.deepcopy(config)\n",
    "\n",
    "        self.generator_config.num_hidden_layers = self.config.generator_num_hidden_layers\n",
    "\n",
    "        self.generator_config.hidden_size //= self.config.multiplier_generator_and_discriminator\n",
    "        self.generator_config.intermediate_size //= self.config.multiplier_generator_and_discriminator\n",
    "        self.generator_config.num_attention_heads //= self.config.multiplier_generator_and_discriminator\n",
    "        \n",
    "        self.generator = ElectraForMaskedLM(self.generator_config)\n",
    "        self.discriminator = ElectraForPreTraining(self.discriminator_config)\n",
    "\n",
    "        if self.config.weight_sharing_degree == 'embedding':\n",
    "            self.embedding = ElectraEmbeddings(self.config)\n",
    "            self.generator.electra.embeddings = self.embedding\n",
    "            self.discriminator.electra.embeddings = self.embedding\n",
    "        \n",
    "        elif self.config.weight_sharing_degree == 'all':\n",
    "            raise NotImplementedError('In case of sharing all weights')\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        print(self.generator)\n",
    "        print(self.discriminator)\n",
    "        \n",
    "        print(\"======== Generator Config ========\")\n",
    "        print(self.generator_config)\n",
    "\n",
    "        print(\"======== Discriminator Config ========\")\n",
    "        print(self.discriminator_config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        masked_lm_labels=None,\n",
    "        logger=None,\n",
    "        global_step=None,\n",
    "    ):\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "        seq_len = input_ids.shape[1]\n",
    "        \n",
    "        # Generator forward\n",
    "        MLM_loss, MLM_prediction_scores = self.generator(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            masked_lm_labels=masked_lm_labels\n",
    "        )\n",
    "        logger.experiment.log_metric('MLM_loss', MLM_loss.item())\n",
    "\n",
    "        ## Prepare discriminator input and RTD labels, adding uniform noise to generator's logits\n",
    "        uniform_noise = torch.rand(MLM_prediction_scores.shape, device=self.device)\n",
    "        gumbel_noise = -(-(uniform_noise + 1e-9).log() + 1e-9).log()\n",
    "\n",
    "        sampled_prediction_scores = MLM_prediction_scores + gumbel_noise\n",
    "        sampled_argmax_words = F.softmax(sampled_prediction_scores, dim=-1).argmax(dim=-1)\n",
    "        sampled_input_ids_to_discriminator = sampled_argmax_words * (masked_lm_labels != -100) + input_ids * (masked_lm_labels == -100)\n",
    "        sampled_RTD_labels = (sampled_input_ids_to_discriminator != masked_lm_labels) * (masked_lm_labels != -100)\n",
    "\n",
    "        # Discriminator Forward\n",
    "        RTD_loss, RTD_prediction_scores = self.discriminator(\n",
    "            input_ids=sampled_input_ids_to_discriminator,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=sampled_RTD_labels,\n",
    "        )\n",
    "        logger.experiment.log_metric('RTD_loss', global_step, RTD_loss.item())\n",
    "\n",
    "\n",
    "        if global_step % self.config.save_log_steps == 0:\n",
    "            # Evaluate Generator's MLM performance based on initial prediction scores\n",
    "            argmax_words = F.softmax(MLM_prediction_scores, dim=-1).argmax(dim=-1)\n",
    "            input_ids_to_discriminator = argmax_words * (masked_lm_labels != -100) + input_ids * (masked_lm_labels == -100) # -> masked 된 토큰들만 generator로 바뀐 것.\n",
    "            RTD_labels = (input_ids_to_discriminator != masked_lm_labels) * (masked_lm_labels != -100)\n",
    "\n",
    "            incorrect_masked_word = RTD_labels.sum().item() # Since True means incorrect word in RTD_labels\n",
    "            total_masked_word = (masked_lm_labels != -100).sum().item()\n",
    "            sampled_incorrect_masked_word = sampled_RTD_labels.sum().item()\n",
    "            logger.experiment.log_metric('MLM_accuracy', global_step, (total_masked_word - incorrect_masked_word) / total_masked_word * 100)\n",
    "            logger.experiment.log_metric('Sampled_MLM_accuracy', global_step, (total_masked_word - sampled_incorrect_masked_word) / total_masked_word * 100)\n",
    "\n",
    "            # Calculate classification metric\n",
    "            RTD_preds = RTD_prediction_scores.sigmoid() >= 0.5\n",
    "            correct_true = ((RTD_preds == RTD_labels) * (RTD_preds == 1)).sum().item()\n",
    "            predicted_true = RTD_preds.sum().item()\n",
    "            target_true = RTD_labels.sum().item()\n",
    "\n",
    "            recall = correct_true / target_true if target_true > 0 else 0\n",
    "            precision = correct_true / predicted_true if predicted_true > 0 else 0\n",
    "            f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "            logger.experiment.log_metric('RTD_accuracy', global_step, (RTD_preds == RTD_labels).sum().item() / (batch_size * seq_len) * 100)\n",
    "            logger.experiment.log_metric('RTD_precision', global_step, precision * 100)\n",
    "            logger.experiment.log_metric('RTD_recall', global_step, recall * 100)\n",
    "            logger.experiment.log_metric('RTD_f1', global_step, f1_score * 100)\n",
    "\n",
    "        loss = MLM_loss + self.config.rtd_loss_weight * RTD_loss\n",
    "        output = (loss, RTD_prediction_scores)\n",
    "        return output\n",
    "\n",
    "    \n",
    "class BaseElectra(pl.LightningModule):\n",
    "    def __init__(self, hparams: argparse.Namespace, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hparams = hparams\n",
    "        self.config = config\n",
    "\n",
    "        self.tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n",
    "        self.model = Electra(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Log steps_per_sec every 100 steps\n",
    "        if self.global_step == 0:\n",
    "            self.last_time = time.time()\n",
    "\n",
    "        elif self.global_step % self.hparams.save_log_steps == 0:\n",
    "            steps_per_sec = self.hparams.save_log_steps / int(time.time() - self.last_time)\n",
    "            examples_per_sec = steps_per_sec * self.hparams.train_batch_size\n",
    "            self.logger.experiment.log_metric('steps_per_sec', self.global_step, steps_per_sec)\n",
    "            self.logger.experiment.log_metric('examples_per_sec', self.global_step, examples_per_sec)\n",
    "            self.logger.experiment.log_metric('learning_rate', self.global_step, self.lr_scheduler.get_last_lr()[-1])\n",
    "            self.last_time = time.time()\n",
    "\n",
    "        input_ids = batch['input_ids']\n",
    "        masked_lm_labels = batch['masked_lm_labels']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        token_type_ids = batch['token_type_ids']\n",
    "\n",
    "        output = self.model(input_ids, attention_mask, token_type_ids, masked_lm_labels, self.logger, self.global_step)\n",
    "        # output = self.model(input_ids, attention_mask, token_type_ids, masked_lm_labels)\n",
    "\n",
    "        self.logger.experiment.log_metric('Total_loss', output[0].item())\n",
    "\n",
    "        # Save model and optimizer\n",
    "        if self.global_step % self.hparams.save_checkpoint_steps == 0 and self.global_step != 0:\n",
    "\n",
    "            ckpt = f'ckpt-{self.global_step:07}'\n",
    "            ckpt_dir = os.path.join(self.hparams.output_dir, ckpt)\n",
    "            generator_dir = os.path.join(ckpt_dir, 'generator')\n",
    "            discriminator_dir = os.path.join(ckpt_dir, 'discriminator')\n",
    "            optimizer_dir = os.path.join(ckpt_dir, 'optimizer.pt')\n",
    "            \n",
    "            os.mkdir(ckpt_dir)\n",
    "            os.mkdir(generator_dir)\n",
    "            os.mkdir(discriminator_dir)\n",
    "\n",
    "            # save artifact to local disk\n",
    "            self.model.generator.save_pretrained(generator_dir)\n",
    "            self.model.discriminator.save_pretrained(discriminator_dir)\n",
    "            torch.save(self.opt.state_dict(), optimizer_dir)\n",
    "\n",
    "            # upload artifact to neptune server\n",
    "            self.logger.log_artifact(os.path.join(generator_dir, 'config.json'), os.path.join(ckpt, 'generator/config.json'))\n",
    "            self.logger.log_artifact(os.path.join(generator_dir, 'pytorch_model.bin'), os.path.join(ckpt, 'generator/pytorch_model.bin'))\n",
    "            self.logger.log_artifact(os.path.join(discriminator_dir, 'config.json'), os.path.join(ckpt, 'discriminator/config.json'))\n",
    "            self.logger.log_artifact(os.path.join(discriminator_dir, 'pytorch_model.bin'), os.path.join(ckpt, 'discriminator/pytorch_model.bin'))\n",
    "            self.logger.log_artifact(os.path.join(ckpt_dir, 'optimizer.pt'), os.path.join(ckpt, 'optimizer.pt'))\n",
    "        \n",
    "        return {'loss': output[0]}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
    "\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]\n",
    "\n",
    "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        self.lr_scheduler.step()\n",
    "\n",
    "    def get_tqdm_dict(self):\n",
    "        avg_loss = getattr(self.trainer, \"avg_loss\", 0.0)\n",
    "        tqdm_dict = {\"loss\": \"{:.3f}\".format(avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
    "        return tqdm_dict\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_batch_size = self.hparams.train_batch_size\n",
    "        \n",
    "        if self.hparams.dataset_type == 'owt':\n",
    "            data_dir = os.path.join(self.hparams.pretrain_dataset_dir, 'openwebtext_lmdb_128')\n",
    "        else:\n",
    "            raise NotImplementedError('Bookcorpus, Wiki dataset is not implemented')\n",
    "\n",
    "        # We should filter out only directory name excluding all the *.tar.gz files\n",
    "        subset_list = [subset_dir for subset_dir in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, subset_dir))]\n",
    "\n",
    "        train_dataset = ConcatDataset([pxt.TorchDataset(os.path.join(data_dir, subset_dir)) for subset_dir in subset_list])\n",
    "        # train_dataset = Subset(train_dataset, range(0, 2))\n",
    "        # assert len(train_dataset) == 48185029, \"length of dataset size is not matched!\" # -> This should be 48185029 lines\n",
    "\n",
    "        # Very small dataset for debugging\n",
    "        # train_dataset = pxt.TorchDataset(os.path.join('../dataset', 'openwebtext_lmdb_128/1_of_14'))\n",
    "        # train_dataset = torch.utils.data.Subset(train_dataset, list(range(0, 1024)))\n",
    "\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer, mlm=self.hparams.mlm, mlm_probability=self.hparams.mlm_probability\n",
    "        )\n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=train_batch_size,\n",
    "            collate_fn=data_collator.collate_batch,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=True,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=self.hparams.max_steps\n",
    "        )\n",
    "        self.lr_scheduler = scheduler\n",
    "        return data_loader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.4.0-py3.6-cuda10.1",
   "language": "python",
   "name": "torch1.4.0-py3.6-cuda10.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
