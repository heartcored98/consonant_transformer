{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch.nn as nn\n",
    "from transformers import AlbertModel, AlbertConfig\n",
    "from transformers.modeling_bert import ACT2FN\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbertConsonantHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(config.embedding_size)\n",
    "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
    "        self.dense = nn.Linear(config.hidden_size, config.embedding_size)\n",
    "        self.decoder = nn.Linear(config.embedding_size, config.output_vocab_size)\n",
    "        self.activation = ACT2FN[config.hidden_act]\n",
    "\n",
    "        self.decoder.bias = self.bias\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.activation(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states)\n",
    "\n",
    "        prediction_scores = hidden_states\n",
    "\n",
    "        return prediction_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Consonant(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Consonant, self).__init__()\n",
    "        self.albert = AlbertModel(config)\n",
    "        self.predictions = AlbertConsonantHead(config) \n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, answer_label=None):\n",
    "        outputs = self.albert(input_ids, attention_mask, token_type_ids)\n",
    "        sequence_output, pooled_output = outputs[:2]\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        \n",
    "        outputs = (prediction_scores) + outputs[2:]  \n",
    "\n",
    "        if answer_label is not None :\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            consonant_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), answer_label.view(-1))\n",
    "            total_loss = consonant_loss\n",
    "            outputs = (total_loss,) + outputs\n",
    "\n",
    "        return outputs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "albert_base_configuration = AlbertConfig(\n",
    "    hidden_size=256,\n",
    "    embedding_size=64,\n",
    "    num_attention_heads=4,\n",
    "    intermediate_size=1024,\n",
    "    vocab_size = 17579,\n",
    "    max_position_embeddings= 100,\n",
    "    output_vocab_size = 589,\n",
    "    type_vocab_size = 1,\n",
    ")\n",
    "\n",
    "model = Consonant(albert_base_configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsonantAlbert(pl.LightningModule):\n",
    "    def __init__(self, hparams: argparse.Namespace, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hparams = hparams\n",
    "        self.config = config\n",
    "\n",
    "        #self.tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n",
    "        self.model = Consonant(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Log steps_per_sec every 100 steps\n",
    "        if self.global_step == 0:\n",
    "            self.last_time = time.time()\n",
    "\n",
    "        elif self.global_step % self.hparams.save_log_steps == 0:\n",
    "            steps_per_sec = self.hparams.save_log_steps / int(time.time() - self.last_time)\n",
    "            examples_per_sec = steps_per_sec * self.hparams.train_batch_size\n",
    "            self.logger.experiment.log_metric('steps_per_sec', self.global_step, steps_per_sec)\n",
    "            self.logger.experiment.log_metric('examples_per_sec', self.global_step, examples_per_sec)\n",
    "            self.logger.experiment.log_metric('learning_rate', self.global_step, self.lr_scheduler.get_last_lr()[-1])\n",
    "            self.last_time = time.time()\n",
    "\n",
    "        input_ids = batch['head_ids']\n",
    "        answer_label = batch['midtail_ids']\n",
    "        attention_mask = batch['attention_masks']\n",
    "\n",
    "        output = self.model(input_ids, attention_mask, answer_label)\n",
    "        # output = self.model(input_ids, attention_mask, token_type_ids, masked_lm_labels)\n",
    "\n",
    "        self.logger.experiment.log_metric('Total_loss', self.global_step, output[0].item())\n",
    "\n",
    "        # Save model and optimizer\n",
    "        if self.global_step % self.hparams.save_checkpoint_steps == 0 and self.global_step != 0:\n",
    "            \n",
    "            ckpt = f'ckpt-{self.global_step:07}.bin'\n",
    "            ckpt_dir = os.path.join(self.hparams.output_dir, ckpt)\n",
    "            \n",
    "            torch.save( {'model_state_dict': self.model.state_dict(), \n",
    "                         'optimizer_state_dict': self.opt.state_dict(),\n",
    "                         'scheduler_state_dict' : self.lr_scheduler.state_dict(),\n",
    "                         'loss': output[0].item()\n",
    "                        }, output_model_file)\n",
    "           \n",
    "            self.logger.log_artifact(ckpt_dir, ckpt_dir)\n",
    "        \n",
    "        return {'loss': output[0]}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch['head_ids']\n",
    "        answer_label = batch['midtail_ids']\n",
    "        attention_mask = batch['attention_masks']\n",
    "\n",
    "        output = self.model(input_ids, attention_mask, answer_label)\n",
    "        logits = output[1]\n",
    "        labels_hat = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        f1, exact = comput_score(answer_label, labels_hat)\n",
    "\n",
    "        output = OrderedDict({\n",
    "            \"val_loss\": output[0].item(),\n",
    "            \"f1\": f1,\n",
    "            \"exact\" : exact\n",
    "            \"batch_size\": len(answer_label)\n",
    "            })\n",
    "        return output\n",
    "\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        val_f1 = sum([out[\"f1\"] for out in outputs]).float() / sum(out[\"batch_size\"] for out in outputs)\n",
    "        val_exact = sum([out[\"f1\"] for out in outputs]).float() / sum(out[\"exact\"] for out in outputs)\n",
    "        val_loss = sum([out[\"val_loss\"] for out in outputs]) / len(outputs)\n",
    "        tqdm_dict = {\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_f1\": val_acc,\n",
    "                \"val_exact\": val_acc\n",
    "                }\n",
    "        result = {\"progress_bar\": tqdm_dict, \"log\": tqdm_dict, \"val_loss\": val_loss}\n",
    "        \n",
    "        self.logger.experiment.log_metric('val_loss', self.global_step, val_loss)\n",
    "        self.logger.experiment.log_metric('val_f1', self.global_step, val_f1)\n",
    "        self.logger.experiment.log_metric('val_exact', self.global_step, val_exact)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
    "\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]\n",
    "\n",
    "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        self.lr_scheduler.step()\n",
    "\n",
    "    def get_tqdm_dict(self):\n",
    "        avg_loss = getattr(self.trainer, \"avg_loss\", 0.0)\n",
    "        tqdm_dict = {\"loss\": \"{:.3f}\".format(avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
    "        return tqdm_dict\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \n",
    "        # We should filter out only directory name excluding all the *.tar.gz files\n",
    "        data_dir = os.path.join(self.hparams.pretrain_dataset_dir, 'train') \n",
    "        subset_list = [subset_dir for subset_dir in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, subset_dir))]\n",
    "        train_dataset = ConcatDataset([pxt.TorchDataset(os.path.join(data_dir, subset_dir)) for subset_dir in subset_list])\n",
    "\n",
    "        # Very small dataset for debugging\n",
    "        # toy_dataset = Subset(train_dataset, range(0, 100)) # -> If you want to make 100sample toy dataset. \n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.hparams.train_batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=True,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=self.hparams.max_steps\n",
    "        )\n",
    "        self.lr_scheduler = scheduler\n",
    "        return data_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \n",
    "        # We should filter out only directory name excluding all the *.tar.gz files\n",
    "        data_dir = os.path.join(self.hparams.pretrain_dataset_dir, 'val') \n",
    "        subset_list = [subset_dir for subset_dir in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, subset_dir))]\n",
    "        train_dataset = ConcatDataset([pxt.TorchDataset(os.path.join(data_dir, subset_dir)) for subset_dir in subset_list])\n",
    "\n",
    "        # Very small dataset for debugging\n",
    "        # toy_dataset = Subset(train_dataset, range(0, 100)) # -> If you want to make 100sample toy dataset. \n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.hparams.train_batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=True,\n",
    "            shuffle=True\n",
    "        )\n",
    "        return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "\n",
    "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
    "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import argparse\n",
    "import shutil\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.logging.neptune import NeptuneLogger\n",
    "from pytorch_lightning.profiler import PassThroughProfiler, AdvancedProfiler\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    set_seed,\n",
    "    ElectraConfig,\n",
    ")\n",
    "\n",
    "\n",
    "from model import BaseElectra\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def revise_config(config: ElectraConfig, args: argparse.Namespace):\n",
    "    \"\"\"\n",
    "    Revise config as we want\n",
    "        1. Add multiplier between generator and discriminator\n",
    "        2. Degree of weight sharing\n",
    "            'no' : Share nothing\n",
    "            'embedding' : Share only embedding layer\n",
    "            'all' : Share all layers\n",
    "        3. Set configuration as electra-small\n",
    "    \"\"\"\n",
    "\n",
    "    config.multiplier_generator_and_discriminator = args.multiplier_generator_and_discriminator\n",
    "    config.weight_sharing_degree = args.weight_sharing_degree\n",
    "    config.rtd_loss_weight = args.rtd_loss_weight\n",
    "    config.generator_num_hidden_layers = args.generator_num_hidden_layers\n",
    "    config.save_log_steps = args.save_log_steps\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def make_parser():\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--mlm', default=True, action='store_true')\n",
    "    parser.add_argument('--mlm_probability', default=0.15, type=float)\n",
    "\n",
    "    parser.add_argument('--learning_rate', default=5e-4, type=float)\n",
    "    parser.add_argument('--weight_decay', default=0.01, type=float)\n",
    "    parser.add_argument('--adam_epsilon', default=1e-6, type=float)\n",
    "    parser.add_argument('--warmup_steps', default=10000, type=int)\n",
    "    parser.add_argument('--train_batch_size', default=128, type=int)\n",
    "    parser.add_argument('--max_grad_norm', default=1.0, type=float)\n",
    "    parser.add_argument('--max_steps', default=1000000, type=int)\n",
    "    parser.add_argument('--save_checkpoint_steps', default=25000, type=int)\n",
    "    parser.add_argument('--save_log_steps', default=100, type=int)\n",
    "\n",
    "    parser.add_argument('--pretrain_dataset_dir', default='../dataset/', type=str)\n",
    "    parser.add_argument('--dataset_type', default='owt', type=str)\n",
    "<<<<<<< HEAD\n",
    "    parser.add_argument(\"--model-name\", default='electra_small_owt',\n",
    "=======\n",
    "    parser.add_argument(\"--model-name\", default='electra_small_owt_reduce_log_metric',\n",
    ">>>>>>> 3ab8eedf19bbc47a4adc9d5fd69671ee451d0959\n",
    "                        help=\"The name of the model being fine-tuned.\")\n",
    "\n",
    "    parser.add_argument('--output_dir', default='output', type=str)\n",
    "    parser.add_argument('--n_gpu', default=1, type=int)\n",
    "    parser.add_argument(\n",
    "        '--gradient_accumulation_steps',\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help='Number of updates steps to accumulate before performing a backward/update pass.',\n",
    "    )\n",
    "    parser.add_argument('--seed', default=42, type=int, help='random seed for initialization')\n",
    "\n",
    "    parser.add_argument('--do_train', action='store_true')\n",
    "    parser.add_argument('--do_eval', action='store_true')\n",
    "\n",
    "    parser.add_argument('--num_workers', default=8, type=int)\n",
    "    parser.add_argument('--generator_num_hidden_layers', default=12, type=int)\n",
    "    parser.add_argument('--discriminator_num_hidden_layers', default=12, type=int)\n",
    "\n",
    "    parser.add_argument('--multiplier_generator_and_discriminator', default=4, type=int)\n",
    "    parser.add_argument('--weight_sharing_degree', default='embedding', type=str)\n",
    "    parser.add_argument('--rtd_loss_weight', default=50.0, type=float)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "def main():\n",
    "\n",
    "    args = make_parser()\n",
    "\n",
    "    config = CONFIG_MAPPING['electra']()\n",
    "    config = revise_config(config, args)\n",
    "\n",
    "    logger.info('Electra config %s', config)\n",
    "    logger.info('Training args %s', args)\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(args.seed)\n",
    "    if not os.path.exists(os.path.join('../', args.output_dir)):\n",
    "        os.mkdir(os.path.join('../', args.output_dir))\n",
    "\n",
    "    # Initialize model directory\n",
    "    # model_dir = \"{}_{}\".format(args.model_name, datetime.datetime.now().strftime(\"%y-%m-%d_%H:%M:%S\"))    \n",
    "    args.output_dir = os.path.join('../', args.output_dir, args.model_name)\n",
    "    if os.path.exists(args.output_dir):\n",
    "        flag_continue = input(f\"Model name [{args.model_name}] already exists. Do you want to overwrite? (y/n): \")\n",
    "        if flag_continue.lower() == 'y' or flag_continue.lower() == 'yes':\n",
    "            shutil.rmtree(args.output_dir)\n",
    "            os.mkdir(args.output_dir)\n",
    "        else:\n",
    "            print(\"Exit pre-training\")\n",
    "            exit()\n",
    "    else:\n",
    "        os.mkdir(args.output_dir)\n",
    "\n",
    "    model = BaseElectra(args, config)\n",
    "    \n",
    "    neptune_api_key = os.environ['NEPTUNE_API_TOKEN']\n",
    "    neptune_project_name = 'IRNLP/electra'\n",
    "    neptune_experiment_name = 'electra_pytorch'\n",
    "\n",
    "    neptune_logger = NeptuneLogger(\n",
    "        api_key=neptune_api_key,\n",
    "        project_name=neptune_project_name,\n",
    "        experiment_name=neptune_experiment_name,\n",
    "        tags=[\"torch\", \"pretrain\"],\n",
    "    )\n",
    "\n",
    "    train_params = dict(\n",
    "        gpus=args.n_gpu,\n",
    "        gradient_clip_val=args.max_grad_norm,\n",
    "        logger=neptune_logger,\n",
    "        early_stop_callback=None,\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(profiler=False, **train_params)\n",
    "    if args.do_train:\n",
    "        trainer.fit(model)\n",
    "\n",
    "    return\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.4.0-py3.6-cuda10.1",
   "language": "python",
   "name": "torch1.4.0-py3.6-cuda10.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}