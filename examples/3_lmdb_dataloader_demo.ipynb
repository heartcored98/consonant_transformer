{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single LMDB Subdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import pyxis.torch as pxt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "44689\npyxis.Reader\nLocation:\t\t'../dataset/processed/ratings_3_100/train/1_of_4'\nNumber of samples:\t44689\nData keys (0th sample):\n\t'head_ids' <- dtype: int32, shape: (100,)\n\t'midtail_ids' <- dtype: int32, shape: (100,)\n\t'attention_masks' <- dtype: bool, shape: (100,)\n"
    }
   ],
   "source": [
    "data_dir = \"../dataset/processed\"\n",
    "subset_dir = \"ratings_3_100/train/1_of_4\"\n",
    "sub_dataset = pxt.TorchDataset(os.path.join(data_dir, subset_dir))\n",
    "\n",
    "print(len(sub_dataset))\n",
    "print(sub_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple LMDB ConcatDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Available Chunk list: ['4_of_16', '7_of_16', '5_of_16', '8_of_16', '1_of_16', '10_of_16', '11_of_16', '3_of_16', '2_of_16', '13_of_16', '14_of_16', '6_of_16', '16_of_16', '9_of_16', '12_of_16', '15_of_16']\nFull Dataset Len: 3943857\n"
    }
   ],
   "source": [
    "\n",
    "train_data_dir = \"../dataset/processed/comments_3_100/val\"\n",
    "\n",
    "# We should filter out only directory name excluding all the *.tar.gz files\n",
    "subset_list = [subset_dir for subset_dir in os.listdir(train_data_dir) if os.path.isdir(os.path.join(train_data_dir, subset_dir))]\n",
    "print('Available Chunk list:', subset_list)\n",
    "\n",
    "full_dataset =  ConcatDataset([pxt.TorchDataset(os.path.join(train_data_dir, subset_dir)) for subset_dir in subset_list])\n",
    "print('Full Dataset Len:', len(full_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Available Chunk list: ['3_of_4', '4_of_4', '2_of_4', '1_of_4']\nFull Dataset Len: 19869\n"
    }
   ],
   "source": [
    "\n",
    "train_data_dir = \"../dataset/processed/ratings_3_100/val\"\n",
    "\n",
    "# We should filter out only directory name excluding all the *.tar.gz files\n",
    "subset_list = [subset_dir for subset_dir in os.listdir(train_data_dir) if os.path.isdir(os.path.join(train_data_dir, subset_dir))]\n",
    "print('Available Chunk list:', subset_list)\n",
    "\n",
    "full_dataset =  ConcatDataset([pxt.TorchDataset(os.path.join(train_data_dir, subset_dir)) for subset_dir in subset_list])\n",
    "print('Full Dataset Len:', len(full_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "keys:  dict_keys(['head_ids', 'midtail_ids', 'attention_masks'])\ntensor([[    1,   209,  5377,  ...,     0,     0,     0],\n        [    1,   483, 12484,  ...,     0,     0,     0],\n        [    1,   379,  9798,  ...,     0,     0,     0],\n        ...,\n        [    1,   549, 14212,  ...,     0,     0,     0],\n        [    1,   237,  6106,  ...,     0,     0,     0],\n        [    1,   360,  9303,  ...,     0,     0,     0]], dtype=torch.int32)\ntorch.Size([128, 100])\ntensor([-100,  246,  225,  190,  253,    0,   29,  337,    2,  561,  561,    0,\n           0,  565,    1,    0,  365,  130,  225,    0,   29,  561,  581,  521,\n           0,    0,  569,  533,    0,  521,  337,  569,  253,    0,   78,   29,\n           5,   30,  561,    0,    1,  569,    1,    0, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100], dtype=torch.int32)\ntorch.Size([128, 100])\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        ...,\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False]])\ntorch.Size([128, 100])\n"
    }
   ],
   "source": [
    "full_dataloader = DataLoader(full_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "for batch in full_dataloader:\n",
    "    print(\"keys: \", batch.keys())\n",
    "\n",
    "    print(batch['head_ids'])\n",
    "    print(batch['head_ids'].shape)\n",
    "\n",
    "    print(batch['midtail_ids'][0])\n",
    "    print(batch['midtail_ids'].shape)\n",
    "\n",
    "    print(batch['attention_masks'])\n",
    "    print(batch['attention_masks'].shape)\n",
    "\n",
    "    \n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-100,  522,  365,  ..., -100, -100, -100],\n        [-100,  561,  533,  ..., -100, -100, -100],\n        [-100,  561,  117,  ..., -100, -100, -100],\n        ...,\n        [-100,  569,  561,  ..., -100, -100, -100],\n        [-100,  190,  253,  ..., -100, -100, -100],\n        [-100,    1,   29,  ...,  141,    0, -100]], dtype=torch.int32)\n4791\n3535\n"
    }
   ],
   "source": [
    "answer_label = batch['midtail_ids']\n",
    "print(answer_label)\n",
    "print(torch.sum(answer_label != -100).item())\n",
    "\n",
    "answer_label[answer_label==0]=-100\n",
    "print(torch.sum(answer_label != -100).item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([-100,  522,  365,  513,  513, -100,   78,    5, -100,  225,  141,  533,\n        -100,    2,  365,  509, -100,  513, -100,  185,  365,   29, -100,  365,\n         133,  117, -100,  132, -100,   26,   21,    1, -100, -100,    1,  521,\n         561, -100,  365, -100,  366, -100,  373,  189,    1, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100], dtype=torch.int32)"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "batch['midtail_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitenvelectraconda1ad35761db5241889e37482be0c5b23a",
   "display_name": "Python 3.6.10 64-bit ('env_electra': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}