{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single LMDB Subdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import pyxis.torch as pxt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "158922\npyxis.Reader\nLocation:\t\t'../dataset/processed/ratings_3_100/train/1_of_1'\nNumber of samples:\t158922\nData keys (0th sample):\n\t'head_ids' <- dtype: int32, shape: (100,)\n\t'midtail_ids' <- dtype: int32, shape: (100,)\n\t'attention_masks' <- dtype: bool, shape: (100,)\n"
    }
   ],
   "source": [
    "data_dir = \"../dataset/processed\"\n",
    "subset_dir = \"ratings_3_100/train/1_of_1\"\n",
    "sub_dataset = pxt.TorchDataset(os.path.join(data_dir, subset_dir))\n",
    "\n",
    "print(len(sub_dataset))\n",
    "print(sub_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple LMDB ConcatDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Available Chunk list: ['1_of_1']\nFull Dataset Len: 158922\n"
    }
   ],
   "source": [
    "\n",
    "train_data_dir = \"../dataset/processed/ratings_3_100/train\"\n",
    "\n",
    "# We should filter out only directory name excluding all the *.tar.gz files\n",
    "subset_list = [subset_dir for subset_dir in os.listdir(train_data_dir) if os.path.isdir(os.path.join(train_data_dir, subset_dir))]\n",
    "print('Available Chunk list:', subset_list)\n",
    "\n",
    "full_dataset =  ConcatDataset([pxt.TorchDataset(os.path.join(train_data_dir, subset_dir)) for subset_dir in subset_list])\n",
    "print('Full Dataset Len:', len(full_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Available Chunk list: ['1_of_1']\nFull Dataset Len: 39727\n"
    }
   ],
   "source": [
    "\n",
    "train_data_dir = \"../dataset/processed/ratings_3_100/val\"\n",
    "\n",
    "# We should filter out only directory name excluding all the *.tar.gz files\n",
    "subset_list = [subset_dir for subset_dir in os.listdir(train_data_dir) if os.path.isdir(os.path.join(train_data_dir, subset_dir))]\n",
    "print('Available Chunk list:', subset_list)\n",
    "\n",
    "full_dataset =  ConcatDataset([pxt.TorchDataset(os.path.join(train_data_dir, subset_dir)) for subset_dir in subset_list])\n",
    "print('Full Dataset Len:', len(full_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "keys:  dict_keys(['head_ids', 'midtail_ids', 'attention_masks'])\ntensor([[    1,   437, 11287,  ...,     0,     0,     0],\n        [    1,   418, 10793,  ...,     0,     0,     0],\n        [    1,    81,     2,  ...,     0,     0,     0],\n        ...,\n        [    1,   250,  6425,  ...,     0,     0,     0],\n        [    1,   247,  6347,  ...,     0,     0,     0],\n        [    1,   478, 12353,  ...,     0,     0,     0]], dtype=torch.int32)\ntorch.Size([128, 100])\ntensor([[-100,   22,  113,  ..., -100, -100, -100],\n        [-100,  117,    5,  ..., -100, -100, -100],\n        [-100,    0, -100,  ..., -100, -100, -100],\n        ...,\n        [-100,  113,  365,  ..., -100, -100, -100],\n        [-100,    1,  365,  ..., -100, -100, -100],\n        [-100,  561,  113,  ..., -100, -100, -100]], dtype=torch.int32)\ntorch.Size([128, 100])\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        ...,\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False]])\ntorch.Size([128, 100])\n"
    }
   ],
   "source": [
    "full_dataloader = DataLoader(full_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "for batch in full_dataloader:\n",
    "    print(\"keys: \", batch.keys())\n",
    "\n",
    "    print(batch['head_ids'])\n",
    "    print(batch['head_ids'].shape)\n",
    "\n",
    "    print(batch['midtail_ids'])\n",
    "    print(batch['midtail_ids'].shape)\n",
    "\n",
    "    print(batch['attention_masks'])\n",
    "    print(batch['attention_masks'].shape)\n",
    "\n",
    "    \n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "horovod0.18.1-tf1.14.0-torch1.2.0-mxnet1.5.0-py3.6-cuda10.0",
   "display_name": "horovod0.18.1-tf1.14.0-torch1.2.0-mxnet1.5.0-py3.6-cuda10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}