{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from consonant.model.tokenization import NGRAMTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Num Head Vocab: 17579\nNum Midtail Vocab: 589\n\n Encoding Example : 하\n=========================\nHead Consonant ID\n0: [PAD], 1: [CLS], 2: [SEP] \n\n[  1 653   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n\nMidtail Consonant ID\n[-100    1 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100]\n\nAttention Mask\n[1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n\n Decoding Example\n=========================\nUnknown consonant replaced to @\n\n하\n"
    }
   ],
   "source": [
    "sentence = [\"내가 너 엄청 좋아해?!이 기호는 불가능$^* 잘릴 예정인 텍스트\", \"너도 나 좋아하니?\"]\n",
    "ntence = [\"내가 너 엄청 좋아해?!이 기호는 불가능$^* 잘릴 예정인 텍스트\", \"너도 나 좋아하니?\"]\n",
    "sentence = [\"하\", \"너도 나 좋아하니?\"]\n",
    "tokenizer = NGRAMTokenizer(3)\n",
    "\n",
    "print(\"Num Head Vocab:\", len(tokenizer.head2id))\n",
    "print(\"Num Midtail Vocab:\", len(tokenizer.midtail2id))\n",
    "\n",
    "result = tokenizer.encode(sentence, max_char_length=30, return_attention_mask=True) #, return_tensors='pt')\n",
    "head_ids = result['head_ids']\n",
    "midtail_ids = result['midtail_ids']\n",
    "attention_masks = result['attention_masks']\n",
    "\n",
    "print('\\n Encoding Example :', sentence[0] )\n",
    "print(\"=========================\")\n",
    "\n",
    "print(\"Head Consonant ID\")\n",
    "print(\"0: [PAD], 1: [CLS], 2: [SEP] \\n\")\n",
    "print(head_ids[0])\n",
    "\n",
    "print()\n",
    "print(\"Midtail Consonant ID\")\n",
    "print(midtail_ids[0])\n",
    "\n",
    "print()\n",
    "print(\"Attention Mask\")\n",
    "print(attention_masks[0])\n",
    "\n",
    "print('\\n Decoding Example')\n",
    "print(\"=========================\")\n",
    "print(\"Unknown consonant replaced to @\\n\")\n",
    "\n",
    "result = tokenizer.decode_sent(head_ids[0], midtail_ids[0])\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bita8f7531970414b1bbdf7938ab7a03f8d",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}