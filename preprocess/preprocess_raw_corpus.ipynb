{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('../data/raw_spoken.txt', header=None, delimiter='\\t') #500k  (499998)\n",
    "df2 = pd.read_csv('../data/raw_ratings.txt', header=None, delimiter='\\t') #200k (199992)\n",
    "df3 = pd.read_csv('../data/raw_wiki_ko_sent.txt', header=None, delimiter='\\t') #4M (4528804)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Corpus Stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Sentences\")\n",
    "print(len(df1))\n",
    "print(len(df2))\n",
    "print(len(df3))\n",
    "\n",
    "df1['length'] = df1[0].apply(len)\n",
    "df2['length'] = df2[0].apply(len)\n",
    "df3['length'] = df3[0].apply(len)\n",
    "\n",
    "print(\"\\n==Basic Stat==\")\n",
    "print(df1['length'].mean(), df1['length'].median(), df1['length'].std())\n",
    "print(df2['length'].mean(), df2['length'].median(), df2['length'].std())\n",
    "print(df3['length'].mean(), df3['length'].median(), df3['length'].std())\n",
    "\n",
    "print(\"\\n==Total Character==\")\n",
    "print(df1['length'].sum())\n",
    "print(df2['length'].sum())\n",
    "print(df3['length'].sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1['length'].hist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2['length'].hist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df3['length'].hist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Initial Consonant\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/clean_corpus.txt', 'r', encoding='utf-8') as input:\n",
    "    sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_char_length = 128\n",
    "\n",
    "a = np.zeros(max_char_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import re\n",
    "\n",
    "\n",
    "class NGRAMTokenizer():\n",
    "\n",
    "    BASE_CODE, HEAD, MID = 44032, 588, 28\n",
    "\n",
    "    # 초성 리스트. 00 ~ 18\n",
    "    HEAD_LIST = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "    EXTRA_LIST = [' ', ',', '.', '?', '!', '~', '∼']\n",
    "\n",
    "    # 중성 리스트. 00 ~ 20\n",
    "    MID_LIST = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ', '@']\n",
    "\n",
    "    # 종성 리스트. 00 ~ 27 + 1(1개 없음)\n",
    "    TAIL_LIST = ['#', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ', '@']\n",
    "\n",
    "    def __init__(self, ngram, max_char_length, head_list=None, mid_list=None, tail_list=None):\n",
    "        self.ngram = ngram\n",
    "        self.max_char_length = max_char_length\n",
    "        self.head_list = head_list if head_list else self.HEAD_LIST\n",
    "        self.mid_list = mid_list if mid_list else self.MID_LIST\n",
    "        self.tail_list = tail_list if tail_list else self.TAIL_LIST\n",
    "\n",
    "        self.head2id = self.generate_head_ngram2id(self.EXTRA_LIST+self.head_list, self.ngram)\n",
    "        self.mid2id = {mid:i for i,mid in enumerate(self.mid_list)}\n",
    "        self.tail2id = {tail:i for i,tail in enumerate(self.tail_list)}\n",
    "\n",
    "    def generate_head_ngram2id(self, head_list, ngram):\n",
    "        ngram_list = list(product(head_list, repeat = ngram))\n",
    "        ngram2id = {ngram_head:i for i,ngram_head in enumerate(ngram_list)}\n",
    "        return ngram2id\n",
    "\n",
    "    def encode(self, sent_list):\n",
    "        encoded_sent_list = list()\n",
    "\n",
    "        list_head_ids = list()\n",
    "        list_mid_ids = list()\n",
    "        list_tail_ids = list()\n",
    "\n",
    "        for sent in sent_list:\n",
    "            head_ids, mid_ids, tail_ids = self.encode_sent(sent)\n",
    "            list_head_ids.append(head_ids)\n",
    "            list_mid_ids.append(mid_ids)\n",
    "            list_tail_ids.append(tail_ids)\n",
    "\n",
    "        return list_head_ids, list_mid_ids, list_tail_ids\n",
    "\n",
    "    def encode_sent(self, sent):\n",
    "        heads = list()\n",
    "        mids = list()\n",
    "        tails = list()\n",
    "\n",
    "        for i, keyword in enumerate(sent[:self.max_char_length]): # truncate with max_char_length\n",
    "            # 한글 여부 check 후 분리\n",
    "            if re.match('.*[ㄱ-ㅎㅏ-ㅣ가-힣]+.*', keyword) is not None:\n",
    "                char_code = ord(keyword) - self.BASE_CODE\n",
    "                char1 = int(char_code / self.HEAD)\n",
    "                heads.append(self.head_list[char1])\n",
    "                #print('초성 : {}'.format(HEAD_LIST[char1]))\n",
    "\n",
    "                char2 = int((char_code - (self.HEAD * char1)) / self.MID)\n",
    "                mids.append(self.mid_list[char2])\n",
    "                #print('중성 : {}'.format(MID_LIST[char2]))\n",
    "\n",
    "                char3 = int((char_code - (self.HEAD * char1) - (self.MID * char2)))\n",
    "                tails.append(self.tail_list[char3])\n",
    "                #print('종성 : {}'.format(TAIL_LIST[char3]))\n",
    "            else: #non-korean character\n",
    "                heads.append(keyword)\n",
    "                mids.append('@')\n",
    "                tails.append('@')\n",
    "\n",
    "        head_ids = np.zeros(self.max_char_length, dtype=np.int)\n",
    "        mid_ids = np.zeros(self.max_char_length, dtype=np.int)\n",
    "        tail_ids = np.zeros(self.max_char_length, dtype=np.int)\n",
    "\n",
    "        # Calculate left, right offset\n",
    "        if self.ngram % 2 == 0: # even ngram\n",
    "            left_offset = (self.ngram) // 2\n",
    "            right_offset = (self.ngram-1) // 2\n",
    "        else: # odd ngram\n",
    "            left_offset = (self.ngram-1) // 2\n",
    "            right_offset = (self.ngram-1) // 2\n",
    "\n",
    "        # Convert consonant to id\n",
    "        for i, (head, mid, tail) in enumerate(zip(heads, mids, tails)):\n",
    "            # ngram-head id\n",
    "            ngram = heads[max(i-left_offset, 0):min(i+right_offset+1, len(heads))]\n",
    "            if i < left_offset:\n",
    "                margin = left_offset - i\n",
    "                ngram = [' '] * margin + ngram\n",
    "            if (len(heads)-1-i) >= 0:\n",
    "                margin = right_offset - (len(heads)-1-i)\n",
    "                ngram = ngram + [' '] * margin \n",
    "\n",
    "            ngram = tuple(ngram)\n",
    "            head_ids[i] = self.head2id[ngram] + 1\n",
    "            mid_ids[i] = self.mid2id[mid] + 1\n",
    "            tail_ids[i] = self.tail2id[tail] + 1\n",
    "\n",
    "        return head_ids, mid_ids, tail_ids\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Num Head Vocab: 17576\nNum  Mid Vocab: 22\nNum Tail Vocab: 29\n\nHead Consonant ID\n-> [  242  6267  4742   235  6103   490 12715 14216   513 13338 12823 17005\n     0     0     0]\n\nMid Consonant ID\n-> [ 2  1 22  5 22  5  5 22  9  1  2 22  0  0  0]\n\nTail Consonant ID\n-> [ 1  1 29  1 29 17 22 29 28  1  1 29  0  0  0]\n"
    }
   ],
   "source": [
    "sentence = [\"내가 너 엄청 좋아해!\"]\n",
    "tokenizer = NGRAMTokenizer(3, 15)\n",
    "\n",
    "print(\"Num Head Vocab:\", len(tokenizer.head2id))\n",
    "print(\"Num  Mid Vocab:\", len(tokenizer.mid2id))\n",
    "print(\"Num Tail Vocab:\", len(tokenizer.tail2id))\n",
    "\n",
    "head_ids, mid_ids, tail_ids = tokenizer.encode(sentence)\n",
    "print()\n",
    "print(\"Head Consonant ID\")\n",
    "print('->', head_ids[0])\n",
    "\n",
    "print()\n",
    "print(\"Mid Consonant ID\")\n",
    "print('->', mid_ids[0])\n",
    "\n",
    "print()\n",
    "print(\"Tail Consonant ID\")\n",
    "print('->', tail_ids[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [\"내가 너 엄청 좋아해!\"]\n",
    "tokenizer = NGRAMTokenizer(3, 15)\n",
    "head_ids, mid_ids, tail_ids = tokenizer.encode(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "grams = [sentence[i:i+N] for i in range(len(sentence)-N+1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitc54cf2991e58487b87798d284fad2dd3",
   "display_name": "Python 3.6.10 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}